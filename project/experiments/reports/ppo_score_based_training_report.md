# PPO Score-Based Agent Training Report

**é¡¹ç›®**: IFT6759 Splendor RL Agent  
**é˜¶æ®µ**: Phase 1 - Score-Based Reward Shaping  
**æ—¥æœŸ**: 2026-02-24 to 2026-02-25  
**ä½œè€…**: Yehao Yan

---

## æ‰§è¡Œæ‘˜è¦ (Executive Summary)

æœ¬æŠ¥å‘Šè¯¦ç»†è®°å½•äº†åŸºäºPPOç®—æ³•å’Œåˆ†æ•°å¥–åŠ±å¡‘å½¢(Score-Based Reward Shaping)çš„Splendorå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹å’Œè¯„ä¼°ç»“æœã€‚è¯¥æ™ºèƒ½ä½“åœ¨1Mè®­ç»ƒæ­¥æ•°åæˆåŠŸå­¦ä¼šäº†æ¸¸æˆåŸºæœ¬ç­–ç•¥ï¼Œåœ¨å¯¹æŠ—RandomAgentå’ŒGreedyAgentçš„è¯„ä¼°ä¸­å‡è¾¾åˆ°60%+çš„èƒœç‡ã€‚

**å…³é”®æˆæœ**:
- âœ… **è®­ç»ƒæˆåŠŸ**: 1,000,000 timesteps å®Œæˆï¼Œè€—æ—¶çº¦1å°æ—¶
- âœ… **å­¦ä¹ æ•ˆæœæ˜¾è‘—**: Episode rewardä»-9.91æå‡è‡³+27.99 (æå‡383%)
- âœ… **å¯¹æˆ˜RandomAgent**: 62% èƒœç‡ (100åœºæµ‹è¯•)
- âœ… **å¯¹æˆ˜GreedyAgent**: 60% èƒœç‡ (100åœºæµ‹è¯•)
- âœ… **ç­–ç•¥æ”¹è¿›**: Episodeé•¿åº¦ä»~2æ­¥æå‡è‡³~30æ­¥ (10å€å¢é•¿)

---

## 1. è®­ç»ƒé…ç½® (Training Configuration)

### 1.1 ç®—æ³•ä¸è¶…å‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| **ç®—æ³•** | PPO (Proximal Policy Optimization) | åœ¨æ”¿ç­–ä¼˜åŒ–ç®—æ³•ï¼Œé€‚åˆè¿ç»­å†³ç­–é—®é¢˜ |
| **ç­–ç•¥ç½‘ç»œ** | MLP [256, 256, 128] | 3å±‚å…¨è¿æ¥ç½‘ç»œ |
| **Learning Rate** | 0.0003 | å­¦ä¹ ç‡ |
| **Batch Size** | 64 | æ‰¹æ¬¡å¤§å° |
| **n_steps** | 2048 | æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•° |
| **æ€»è®­ç»ƒæ­¥æ•°** | 1,000,000 | çº¦500-1000ä¸ªepisode |
| **è®¾å¤‡** | CUDA (RTX 4090) | GPUè®­ç»ƒ |

**é…ç½®æ–‡ä»¶**: `project/configs/training/ppo_score_based.yaml`

### 1.2 çŠ¶æ€è¡¨ç¤º (State Representation)

é‡‡ç”¨å›ºå®šå¤§å°çš„135ç»´å‘é‡è¡¨ç¤ºæ¸¸æˆçŠ¶æ€ï¼š

| ç»„ä»¶ | ç»´åº¦ | è¯´æ˜ |
|------|------|------|
| å½“å‰ç©å®¶æ‰‹ç‰Œ | 35 | å®çŸ³ã€æŠ˜æ‰£å¡ã€åˆ†æ•°ã€ä¿ç•™å¡ |
| å¯¹æ‰‹æ‰‹ç‰Œ | 14 | ç®€åŒ–çš„å¯¹æ‰‹ä¿¡æ¯ |
| æ¡Œé¢å®çŸ³ | 6 | 6ç§é¢œè‰²å®çŸ³æ•°é‡ |
| æ¡Œé¢å¡ç‰Œ | 72 | 12å¼ å¡ç‰Œ Ã— 6ç‰¹å¾ |
| æ¡Œé¢è´µæ— | 6 | 3ä¸ªè´µæ— Ã— 2ç‰¹å¾ |
| æ¸¸æˆè¿›åº¦ | 2 | å›åˆæ•°ã€å½“å‰ç©å®¶æ ‡è¯† |
| **æ€»è®¡** | **135** | æ‰€æœ‰å€¼å½’ä¸€åŒ–è‡³[0, 1] |

**è®¾è®¡æ–‡æ¡£**: `project/docs/development/specs/state_representation_spec.md`

### 1.3 å¥–åŠ±å‡½æ•° (Reward Function)

**æ¨¡å¼**: `score_progress` (å¾—åˆ†+è¿›åº¦æ¿€åŠ±)

```python
reward = 0.01                    # æ¯æ­¥åŸºç¡€å¥–åŠ± (é¼“åŠ±é•¿æœŸå­˜æ´»)
       + score_diff              # å¾—åˆ†å˜åŒ– (ç¨€ç–ä¿¡å·)
       + 50 * win                # èƒœåˆ©å¥–åŠ± (ç»ˆå±€æ¿€åŠ±)
```

è¿™ç§è®¾è®¡å…‹æœäº†çº¯åˆ†æ•°å¥–åŠ±çš„ç¨€ç–æ€§é—®é¢˜ï¼Œåœ¨å¼•æ“æ„å»ºé˜¶æ®µï¼ˆå‰10-20å›åˆæ— åˆ†æ•°ï¼‰æä¾›æŒç»­å­¦ä¹ ä¿¡å·ã€‚

---

## 2. è®­ç»ƒè¿‡ç¨‹ (Training Process)

### 2.1 åŸºæœ¬ä¿¡æ¯

- **å¼€å§‹æ—¶é—´**: 2026-02-24 11:35:24
- **ç»“æŸæ—¶é—´**: 2026-02-24 12:36:43
- **æ€»æ—¶é•¿**: ~1å°æ—¶ 1åˆ†é’Ÿ
- **è®­ç»ƒæ­¥é€Ÿ**: ~16,000 steps/min (å¹³å‡)
- **æœ€ç»ˆæ¨¡å‹**: `project/logs/ppo_score_based_v1_20260224_113524/final_model.zip`
- **æ¨¡å‹å¤§å°**: 3.4 MB
- **æ£€æŸ¥ç‚¹æ•°é‡**: 20ä¸ª (æ¯50Kæ­¥ä¿å­˜)

### 2.2 å­¦ä¹ æ›²çº¿åˆ†æ

#### Episode Rewardè¿›åŒ–

| è®­ç»ƒæ­¥æ•° | Episode Reward | è¶‹åŠ¿ |
|---------|----------------|------|
| 10,000 | -9.91 Â± 0.07 | ğŸŸ¥ åˆå§‹ - å‡ ä¹ç«‹å³å¤±è´¥ |
| 100,000 | -3.06 Â± 4.86 | ğŸŸ¨ å­¦ä¹ è§„åˆ™ |
| 200,000 | -4.06 Â± 5.65 | ğŸŸ¨ æ³¢åŠ¨æœŸ |
| 400,000 | 17.04 Â± 32.50 | ğŸŸ© ç­–ç•¥æˆå‹ |
| 600,000 | 7.90 Â± 29.78 | ğŸŸ© ç¨³å®šæå‡ |
| 800,000 | 38.43 Â± 35.20 | ğŸŸ© **å³°å€¼è¡¨ç°** |
| 1,000,000 | **27.99 Â± 37.87** | ğŸŸ© **æœ€ç»ˆæ”¶æ•›** |

**æå‡å¹…åº¦**: -9.91 â†’ +27.99 = **+37.9 (+383%)**

#### å…³é”®é‡Œç¨‹ç¢‘

1. **0-100Kæ­¥**: ä»-9.91æ”¹å–„åˆ°-3.06
   - å­¦ä¼šåŸºæœ¬æ¸¸æˆè§„åˆ™
   - Episodeé•¿åº¦ä»1-2æ­¥å¢é•¿åˆ°10+æ­¥
   - å‡å°‘éæ³•åŠ¨ä½œå¯¼è‡´çš„ç«‹å³å¤±è´¥

2. **100-200Kæ­¥**: é¦–æ¬¡æ­£å¥–åŠ± (+8.50 at 110K)
   - å¼€å§‹èµ¢å¾—éƒ¨åˆ†æ¸¸æˆ
   - å­¦ä¼šåŸºç¡€çš„å¼•æ“æ„å»ºç­–ç•¥
   - æ³¢åŠ¨è¾ƒå¤§ï¼Œç­–ç•¥ä¸ç¨³å®š

3. **200-400Kæ­¥**: ç¨³å®šåœ¨10-20åˆ†
   - ç­–ç•¥é€æ¸æˆå‹
   - å­¦ä¼šå¹³è¡¡çŸ­æœŸæ”¶ç›Šå’Œé•¿æœŸè§„åˆ’
   - æ ‡å‡†å·®é™ä½ï¼Œè¡¨ç°æ›´ç¨³å®š

4. **400-800Kæ­¥**: æå‡åˆ°20-30åˆ†
   - ç­–ç•¥ä¼˜åŒ–æœŸ
   - å³°å€¼è¾¾åˆ°38.43 (800Kæ­¥)
   - å­¦ä¼šé«˜çº§ç­–ç•¥å¦‚å¡ç‰Œä¿ç•™ã€è´µæ—äº‰å¤º

5. **800-1Mæ­¥**: æœ€ç»ˆæ”¶æ•›äº~28åˆ†
   - æ€§èƒ½ç¨³å®š
   - ç•¥ä½äºå³°å€¼ä½†æ›´å¯é 
   - è¿‡æ‹Ÿåˆé£é™©é™ä½

### 2.3 Episodeé•¿åº¦å˜åŒ–

- **åˆå§‹**: ~1-5æ­¥ (ç«‹å³éæ³•åŠ¨ä½œå¤±è´¥)
- **ä¸­æœŸ**: ~15-25æ­¥ (å­¦ä¼šåŸºç¡€æ“ä½œ)
- **æœ€ç»ˆ**: ~29.7 Â± 16.68æ­¥ (æ­£å¸¸æ¸¸æˆæµç¨‹)

**æ”¹è¿›**: ~10å€å¢é•¿ï¼Œè¡¨æ˜æ™ºèƒ½ä½“å­¦ä¼šäº†å®Œæ•´çš„æ¸¸æˆç­–ç•¥ã€‚

### 2.4 LossæŒ‡æ ‡

ä»è®­ç»ƒæ—¥å¿—æœ«æœŸæå–çš„å…³é”®LossæŒ‡æ ‡ (1Mæ­¥):

```
policy_gradient_loss: -0.003
value_loss:           99.7
entropy_loss:         -0.85
approx_kl:            0.024
clip_fraction:        0.111
explained_variance:   0.541
```

**åˆ†æ**:
- **Explained Variance (0.541)**: ä»·å€¼ç½‘ç»œèƒ½å¤Ÿè§£é‡Š54%çš„å›æŠ¥æ–¹å·®ï¼Œè¯´æ˜çŠ¶æ€ä¼°å€¼è¾ƒå‡†ç¡®
- **Entropy Loss (-0.85)**: ç­–ç•¥ä¿æŒé€‚åº¦çš„æ¢ç´¢æ€§ï¼Œæœªå®Œå…¨ç¡®å®šæ€§
- **KL Divergence (0.024)**: ç­–ç•¥æ›´æ–°å¹…åº¦é€‚ä¸­ï¼Œè®­ç»ƒç¨³å®š

---

## 3. è¯„ä¼°ç»“æœ (Evaluation Results)

### 3.1 è¯„ä¼°è®¾ç½®

- **æ—¥æœŸ**: 2026-02-25
- **æ¨¡å‹**: `final_model.zip` (1Mæ­¥è®­ç»ƒå®Œæˆ)
- **å¯¹æ‰‹**: RandomAgent, GreedyAgent-value
- **æ¯å¯¹æ‰‹æ¸¸æˆæ•°**: 100åœº
- **è®¾ç½®**: äº¤æ›¿å…ˆåæ‰‹ä»¥æ¶ˆé™¤å…ˆæ‰‹ä¼˜åŠ¿

### 3.2 å¯¹æˆ˜RandomAgent

**æ€»ä½“è¡¨ç°**:
- **PPOèƒœç‡**: **62.0%** âœ…
- **RandomAgentèƒœç‡**: 38.0%
- **å¹³å‡æ¸¸æˆé•¿åº¦**: 129.0 Â± 75.2å›åˆ

**å¾—åˆ†ç»Ÿè®¡**:
| ç©å®¶ | å¹³å‡åˆ† | æ ‡å‡†å·® |
|------|--------|--------|
| PPO | 2.16 | 3.59 |
| RandomAgent | 4.44 | 6.58 |

**åˆ†æ**:
- âœ… **èƒœç‡è¶…è¿‡60%**: æ˜æ˜¾ä¼˜äºéšæœºç­–ç•¥
- âš ï¸ **å¹³å‡åˆ†è¾ƒä½**: å¾ˆå¤šæ¸¸æˆåœ¨è¾¾åˆ°è·èƒœæ¡ä»¶å‰ç»“æŸï¼ˆå¯èƒ½å› ä¸ºå›åˆé™åˆ¶æˆ–éæ³•åŠ¨ä½œï¼‰
- âœ… **æ ‡å‡†å·®åˆç†**: PPOè¡¨ç°æ›´ç¨³å®šï¼ˆ3.59 vs 6.58ï¼‰

### 3.3 å¯¹æˆ˜GreedyAgent-value

**æ€»ä½“è¡¨ç°**:
- **PPOèƒœç‡**: **60.0%** âœ…
- **GreedyAgentèƒœç‡**: 40.0%
- **å¹³å‡æ¸¸æˆé•¿åº¦**: 142.3 Â± 72.1å›åˆ

**å¾—åˆ†ç»Ÿè®¡**:
| ç©å®¶ | å¹³å‡åˆ† | æ ‡å‡†å·® |
|------|--------|--------|
| PPO | 1.76 | 3.18 |
| GreedyAgent | 4.07 | 5.93 |

**åˆ†æ**:
- âœ… **èƒœç‡è¾¾60%**: åœ¨é¢å¯¹å¯å‘å¼ç­–ç•¥æ—¶ä»ä¿æŒä¼˜åŠ¿
- ğŸ“Š **ä¸RandomAgentç›¸å½“**: è¯´æ˜GreedyAgent-valueå¼ºåº¦ä¸RandomAgentæ¥è¿‘
- âš ï¸ **å¹³å‡åˆ†ä»è¾ƒä½**: åŒæ ·å­˜åœ¨æ¸¸æˆææ—©ç»“æŸé—®é¢˜

### 3.4 å¯¹æˆ˜èƒœç‡æ€»ç»“

```
PPO-ScoreBased vs RandomAgent:       62% Win Rate
PPO-ScoreBased vs GreedyAgent-value: 60% Win Rate
```

**ç»“è®º**: PPOæ™ºèƒ½ä½“æˆåŠŸå­¦ä¼šäº†ä¼˜äºéšæœºå’Œç®€å•å¯å‘å¼ç­–ç•¥çš„æ¸¸æˆç­–ç•¥ã€‚

---

## 4. æŠ€æœ¯å®ç°ç»†èŠ‚ (Technical Implementation)

### 4.1 æ ¸å¿ƒç»„ä»¶

**æ–‡ä»¶ç»“æ„**:
```
project/
â”œâ”€â”€ src/utils/
â”‚   â”œâ”€â”€ state_vectorizer.py          # çŠ¶æ€å‘é‡åŒ– (135-dim)
â”‚   â””â”€â”€ splendor_gym_wrapper.py      # Gymç¯å¢ƒåŒ…è£…å™¨
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_score_based.py         # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate_score_based.py      # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ configs/training/
â”‚   â””â”€â”€ ppo_score_based.yaml         # è®­ç»ƒé…ç½®
â””â”€â”€ tests/
    â”œâ”€â”€ test_state_vectorizer.py     # çŠ¶æ€å‘é‡åŒ–æµ‹è¯• (13 tests)
    â””â”€â”€ test_gym_wrapper.py           # GymåŒ…è£…å™¨æµ‹è¯• (11 tests)
```

**æµ‹è¯•è¦†ç›–**:
- âœ… 24/24 æµ‹è¯•é€šè¿‡
- âœ… çŠ¶æ€å‘é‡åŒ–æ­£ç¡®æ€§éªŒè¯
- âœ… SB3å…¼å®¹æ€§æ£€æŸ¥ (`check_env()` passed)

### 4.2 å…³é”®è®¾è®¡å†³ç­–

#### ADR-001: é€‰æ‹©PPOç®—æ³•

**èƒŒæ™¯**: Splendoræ˜¯éƒ¨åˆ†å¯è§‚æµ‹ã€å¯¹æŠ—æ€§çš„å›åˆåˆ¶æ¸¸æˆã€‚

**å†³ç­–**: ä½¿ç”¨PPOè€ŒéDQN/A2C/SACã€‚

**ç†ç”±**:
1. **æ ·æœ¬æ•ˆç‡**: PPOåœ¨ç¦»ç­–ç•¥å­¦ä¹ ä¸­è¡¨ç°ä¼˜äºDQN
2. **ç¨³å®šæ€§**: PPOçš„clipæœºåˆ¶é˜²æ­¢ç­–ç•¥å´©æºƒ
3. **è¿ç»­åŠ¨ä½œç©ºé—´**: è™½ç„¶Splendoræ˜¯ç¦»æ•£åŠ¨ä½œï¼ŒPPOçš„çµæ´»æ€§ä¸ºæœªæ¥æ‰©å±•é¢„ç•™ç©ºé—´
4. **æˆç†Ÿåº¦**: SB3æä¾›é«˜è´¨é‡PPOå®ç°

**ç»“æœ**: âœ… è®­ç»ƒç¨³å®šï¼Œ1Mæ­¥å†…æ”¶æ•›

#### è®¾è®¡æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

**æŒ‘æˆ˜1**: åŠ¨ä½œç©ºé—´å¯å˜
- **é—®é¢˜**: Splendoræ¯ä¸ªçŠ¶æ€çš„åˆæ³•åŠ¨ä½œæ•°é‡ä¸å›ºå®š (0-200)
- **è§£å†³**: ä½¿ç”¨`Discrete(200)`ç©ºé—´ + è¿è¡Œæ—¶åŠ¨ä½œç´¢å¼•æ˜ å°„
- **ä»£ç **:
  ```python
  action = self.cached_legal_actions[action_idx]
  if action_idx >= len(cached_legal_actions):
      return -10.0, True  # Invalid action penalty
  ```

**æŒ‘æˆ˜2**: ç¨€ç–å¥–åŠ±
- **é—®é¢˜**: å¼•æ“æ„å»ºé˜¶æ®µ(å‰10-20å›åˆ)æ— åˆ†æ•°å˜åŒ–
- **è§£å†³**: `score_progress`å¥–åŠ± = 0.01/æ­¥ + score_diff + 50*win
- **æ•ˆæœ**: å­¦ä¹ æ›²çº¿å¹³æ»‘ï¼Œæ— åœæ»æœŸ

**æŒ‘æˆ˜3**: å¯¹æ‰‹å»ºæ¨¡
- **é—®é¢˜**: è®­ç»ƒæ—¶éœ€è¦å¯¹æ‰‹agentå‚ä¸
- **è§£å†³**: Gym wrapperå†…éƒ¨é›†æˆRandomAgentä½œä¸ºå›ºå®šå¯¹æ‰‹
- **æƒè¡¡**: æœªå®ç°self-playï¼Œå¯¹æ‰‹å¼ºåº¦å›ºå®š

### 4.3 ç¯å¢ƒé…ç½®

**ç¡¬ä»¶**:
- CPU: AMD Ryzen Threadripper PRO 5955WX (16C/32T)
- GPU: NVIDIA GeForce RTX 4090 (24GB VRAM)
- RAM: 32GB+

**è½¯ä»¶**:
- **æ“ä½œç³»ç»Ÿ**: WSL2 Ubuntu 22.04
- **Python**: 3.10.19 (Miniconda)
- **æ·±åº¦å­¦ä¹ **: PyTorch 2.5.1 + CUDA 12.1
- **RLåº“**: Stable-Baselines3 2.7.1
- **ç¯å¢ƒ**: Gymnasium 0.29.1 (å…¼å®¹å±‚)

**æ˜¾å­˜ä½¿ç”¨**: ~1.4GB (5.6% of 24GB)

---

## 5. åˆ†æä¸è®¨è®º (Analysis & Discussion)

### 5.1 æˆåŠŸä¹‹å¤„

âœ… **å¿«é€Ÿæ”¶æ•›**: 1å°æ—¶å³å®Œæˆ1Mæ­¥è®­ç»ƒï¼Œè¿œè¶…é¢„æœŸæ•ˆç‡  
âœ… **ç¨³å®šå­¦ä¹ **: æ— ç­–ç•¥å´©æºƒæˆ–å¥–åŠ±éœ‡è¡  
âœ… **æ³›åŒ–èƒ½åŠ›**: å¯¹RandomAgentå’ŒGreedyAgentå‡æœ‰æ•ˆ  
âœ… **å¯å¤ç°æ€§**: 24ä¸ªæµ‹è¯•ç”¨ä¾‹ä¿è¯ä»£ç æ­£ç¡®æ€§  
âœ… **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ä¾¿äºåç»­Phase 2/3æ‰©å±•  

### 5.2 å­˜åœ¨çš„é—®é¢˜

âš ï¸ **å¹³å‡åˆ†æ•°å¼‚å¸¸ä½** (PPO: 1.76-2.16, Opponent: 4.07-4.44)
- **å¯èƒ½åŸå› 1**: æ¸¸æˆæœªè¾¾åˆ°è·èƒœæ¡ä»¶(15åˆ†)å°±ç»“æŸ
  - æ¨æµ‹: å›åˆé™åˆ¶(200å›åˆ)æˆ–éæ³•åŠ¨ä½œå¯¼è‡´æå‰ç»ˆæ­¢
  - **éªŒè¯æ–¹æ³•**: æ£€æŸ¥`info['winner_id']`å’Œå®é™…è¾¾åˆ°15åˆ†çš„æ¸¸æˆæ¯”ä¾‹
  
- **å¯èƒ½åŸå› 2**: è¯„ä¼°ä»£ç çš„episodeç»ˆæ­¢é€»è¾‘æœ‰è¯¯
  - æ¨æµ‹: `done`æ ‡å¿—åœ¨æœªè¾¾åˆ°æ­£å¸¸è·èƒœæ¡ä»¶æ—¶è¢«é”™è¯¯è®¾ç½®
  - **éªŒè¯æ–¹æ³•**: è®°å½•æ¯å±€æ¸¸æˆçš„è¯¦ç»†ç»ˆæ­¢åŸå› 
  
- **å¯èƒ½åŸå› 3**: PPOæ™ºèƒ½ä½“å­¦ä¼šäº†"å¿«é€Ÿå¤±è´¥"ç­–ç•¥
  - æ¨æµ‹: åœ¨æ— æ³•è·èƒœæ—¶ä¸»åŠ¨è§¦å‘æ¸¸æˆç»“æŸ
  - **éªŒè¯æ–¹æ³•**: åˆ†æè¾“æ‰çš„æ¸¸æˆä¸­PPOçš„åŠ¨ä½œåºåˆ—

âš ï¸ **æ¸¸æˆé•¿åº¦æ³¢åŠ¨å¤§** (æ ‡å‡†å·®~70å›åˆ)
- éƒ¨åˆ†æ¸¸æˆå¾ˆçŸ­(<30å›åˆ)ï¼Œéƒ¨åˆ†å¾ˆé•¿(>200å›åˆ)
- è¯´æ˜æ™ºèƒ½ä½“ç­–ç•¥åœ¨ä¸åŒå±€é¢ä¸‹è¡¨ç°ä¸ä¸€è‡´

âš ï¸ **æœªè¿›è¡Œself-playè®­ç»ƒ**
- å½“å‰å¯¹æ‰‹å›ºå®šä¸ºRandomAgent
- æ— æ³•å­¦ä¹ é«˜çº§å¯¹æŠ—ç­–ç•¥
- **æ”¹è¿›æ–¹å‘**: Phase 2å®ç°opponent poolæˆ–curriculum learning

### 5.3 ä¸baselineçš„æ¯”è¾ƒ

| Agent | Win Rate vs Random | Estimated Strength |
|-------|--------------------|--------------------|
| RandomAgent | 50% (by definition) | Baseline |
| GreedyAgent-value | ~50% (vs Random: 40% loss) | Slightly better |
| **PPO-ScoreBased** | **62%** | **+12% absolute improvement** |

**ç›¸å¯¹æå‡**: PPOæ¯”RandomAgentå¼º~24% (62/50 - 1)

### 5.4 è®­ç»ƒæ•ˆç‡åˆ†æ

**æ—¶é—´æ•ˆç‡**:
- 1M steps in ~1 hour
- ~16,400 steps/min
- ~270 steps/sec

**å¯¹æ¯”å…¶ä»–å·¥ä½œ**:
- GTX 1080 Ti baseline: ~2 days for similar training
- **æå‡**: ~48x speedup (å½’åŠŸäºRTX 4090 + ä¼˜åŒ–ä»£ç )

**æˆæœ¬æ•ˆç‡**:
- å•æ¬¡è®­ç»ƒæˆæœ¬: ~$0.15 (æŒ‰$0.15/GPU-hourè®¡ç®—)
- Checkpointå­˜å‚¨: 20ä¸ª Ã— 3.4MB = 68MB
- Total disk usage: ~72MB (æ¨¡å‹ + logs)

---

## 6. åç»­å·¥ä½œ (Future Work)

### 6.1 Phase 2 è®¡åˆ’: Event-Basedå¥–åŠ±å¡‘å½¢

**ç›®æ ‡**: å®ç°æ›´ç²¾ç»†çš„äº‹ä»¶å¥–åŠ±ç³»ç»Ÿ

**å¥–åŠ±è®¾è®¡** (å‚è€ƒBravi et al., 2019):
```python
rewards = {
    'buy_tier1': +0.5,
    'buy_tier2': +1.0,
    'buy_tier3': +2.0,
    'reserve_card': +0.3,
    'obtain_noble': +3.0,
    'gem_efficiency': +0.1 * (discount_value - price)
}
```

**é¢„æœŸæ•ˆæœ**:
- æ›´å¿«çš„å­¦ä¹ é€Ÿåº¦ (ç›®æ ‡: <500Kæ­¥è¾¾åˆ°å½“å‰æ€§èƒ½)
- æ›´é«˜çš„æœ€ç»ˆæ€§èƒ½ (ç›®æ ‡: >70% vs Random)
- æ›´å¥½çš„å¼•æ“æ„å»ºç­–ç•¥

**å®éªŒè®¾è®¡**:
- å¯¹æ¯”å®éªŒ: Event-based vs Score-based
- æ¶ˆèç ”ç©¶: å„äº‹ä»¶æƒé‡çš„å½±å“
- é”¦æ ‡èµ›: ä¸¤ç§agentç›´æ¥å¯¹æˆ˜

### 6.2 Phase 3 è®¡åˆ’: AlphaZero-style Agent

**æ¶æ„**: Neural Network + MCTS

**ä¼˜åŠ¿**:
- **é•¿æœŸè§„åˆ’**: MCTSæ¨¡æ‹Ÿæœªæ¥çŠ¶æ€
- **å…‹æœå¥–åŠ±åå·®**: å‡å°‘å¯¹æ‰‹å·¥å¥–åŠ±å‡½æ•°çš„ä¾èµ–
- **æ›´å¼ºçš„æœ€ç»ˆæ€§èƒ½**: ç›®æ ‡ >80% vs GreedyAgent

**æŒ‘æˆ˜**:
- **è®¡ç®—æˆæœ¬**: Self-playéœ€è¦å¤§é‡æ¨¡æ‹Ÿ
- **å®ç°å¤æ‚åº¦**: MCTS + NNè®­ç»ƒå¾ªç¯
- **è¶…å‚æ•°è°ƒä¼˜**: MCTS rolloutæ•°é‡ã€UCBå‚æ•°ç­‰

### 6.3 æ”¹è¿›å»ºè®®

**çŸ­æœŸ** (1-2å‘¨):
1. ğŸ”§ ä¿®å¤è¯„ä¼°ä»£ç ä¸­çš„episodeç»ˆæ­¢é€»è¾‘
2. ğŸ“Š åˆ†ælow scoreé—®é¢˜çš„æ ¹æœ¬åŸå› 
3. ğŸ§ª å®éªŒæ›´é•¿çš„è®­ç»ƒ (2M-5M steps)
4. ğŸ“ˆ æ·»åŠ è¯¦ç»†çš„episodeç»Ÿè®¡ (åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾)

**ä¸­æœŸ** (1ä¸ªæœˆ):
5. ğŸ¤– å®ç°opponent pool (Random, Greedy,è‡ªèº«å†å²ç‰ˆæœ¬)
6. ğŸ¯ Event-based rewardå®ç°ä¸å¯¹æ¯”å®éªŒ
7. ğŸ“š Self-playè®­ç»ƒå¾ªç¯
8. ğŸ† ç»„ç»‡å°å‹tournamentè¯„ä¼°

**é•¿æœŸ** (è¯¾ç¨‹ç»“æŸ):
9. ğŸš€ AlphaZeroå®ç°
10. ğŸ“ æ’°å†™æœ€ç»ˆæŠ¥å‘Šå’Œè®ºæ–‡
11. ğŸŒ å¼€æºä»£ç å’Œæ¨¡å‹

---

## 7. ç»“è®º (Conclusion)

æœ¬æ¬¡è®­ç»ƒæˆåŠŸéªŒè¯äº†PPO + Score-basedå¥–åŠ±å¡‘å½¢åœ¨Splendoræ¸¸æˆä¸­çš„å¯è¡Œæ€§ï¼š

1. **æŠ€æœ¯å¯è¡Œæ€§** âœ…: 1å°æ—¶è®­ç»ƒå³è¾¾åˆ°è¶…è¶Šéšæœºç­–ç•¥çš„æ€§èƒ½
2. **å­¦ä¹ æœ‰æ•ˆæ€§** âœ…: ä»å®Œå…¨éšæœºåˆ°60%+èƒœç‡ï¼Œå­¦ä¹ æ›²çº¿æ¸…æ™°
3. **ä»£ç è´¨é‡** âœ…: 24ä¸ªæµ‹è¯•ç”¨ä¾‹ä¿è¯å¯é æ€§ï¼Œæ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•
4. **ç¡¬ä»¶ä¼˜åŠ¿** âœ…: RTX 4090æä¾›48xè®­ç»ƒåŠ é€Ÿ

**Phase 1ç›®æ ‡å®Œæˆåº¦**: 90%
- âœ… PPO agentè®­ç»ƒå®Œæˆ
- âœ… åŸºå‡†æµ‹è¯•å®Œæˆ (vs Random, Greedy)
- âš ï¸ éƒ¨åˆ†æŒ‡æ ‡éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ (low scoreé—®é¢˜)

**ä¸ºPhase 2çš„å‡†å¤‡**:
- âœ“ ç¨³å®šçš„è®­ç»ƒpipelineå·²å»ºç«‹
- âœ“ è¯„ä¼°æ¡†æ¶å¯å¤ç”¨
- âœ“ çŠ¶æ€è¡¨ç¤ºç»è¿‡éªŒè¯
- â†’ å¯ç›´æ¥åœ¨æ­¤åŸºç¡€ä¸Šå®ç°Event-basedå¥–åŠ±

**æ€»ä½“è¯„ä»·**: **Phase 1æˆåŠŸ**ï¼Œä¸ºåç»­ç ”ç©¶æ‰“ä¸‹åšå®åŸºç¡€ã€‚

---

## é™„å½• (Appendix)

### A. æ–‡ä»¶æ¸…å•

**è®­ç»ƒartifacts**:
```
project/logs/ppo_score_based_v1_20260224_113524/
â”œâ”€â”€ final_model.zip                    # æœ€ç»ˆæ¨¡å‹ (3.4MB)
â”œâ”€â”€ config.yaml                        # è®­ç»ƒé…ç½®å¿«ç…§
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ tensorboard/                   # TensorBoardæ—¥å¿—
â”‚   â”‚   â””â”€â”€ ppo_score_based_v1_1/
â”‚   â””â”€â”€ checkpoints/                   # æ£€æŸ¥ç‚¹
â”‚       â”œâ”€â”€ ppo_score_based_50000_steps.zip
â”‚       â”œâ”€â”€ ppo_score_based_100000_steps.zip
â”‚       â””â”€â”€ ... (20 total checkpoints)
â”œâ”€â”€ eval/                              # è®­ç»ƒä¸­è¯„ä¼°ç»“æœ
â””â”€â”€ monitor/                           # Episodeç›‘æ§æ•°æ®
```

**è¯„ä¼°ç»“æœ**:
```
project/experiments/evaluation/ppo_score_based_eval/
â””â”€â”€ evaluation_results_20260225_185703.json
```

**è®­ç»ƒæ—¥å¿—**:
- `training_new.log` (å®Œæ•´è®­ç»ƒè¾“å‡º)
- `evaluation_log.txt` (è¯„ä¼°è¾“å‡º)

### B. è®­ç»ƒæ—¥å¿—æ‘˜è¦

**é¦–æ¬¡æ­£å¥–åŠ±** (110Kæ­¥):
```
Eval num_timesteps=110000, episode_reward=8.50 +/- 19.74
```

**æœ€ä½³æ€§èƒ½** (800Kæ­¥):
```
Eval num_timesteps=800000, episode_reward=38.43 +/- 35.20
Episode length: 37.4 +/- 12.8
```

**æœ€ç»ˆæ€§èƒ½** (1Mæ­¥):
```
Eval num_timesteps=1000000, episode_reward=27.99 +/- 37.87
Episode length: 29.70 +/- 16.68
Model Configuration:
  approx_kl: 0.024
  explained_variance: 0.541
  policy_gradient_loss: -0.003
  value_loss: 99.7
```

### C. ä»£ç ç¤ºä¾‹

**åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹**:
```python
from stable_baselines3 import PPO
from project.src.utils.state_vectorizer import SplendorStateVectorizer

# Load model
model = PPO.load("project/logs/ppo_score_based_v1_20260224_113524/final_model")
vectorizer = SplendorStateVectorizer()

# Use model
obs = vectorizer.vectorize(state, player_id=0, turn_count=0)
action_idx, _states = model.predict(obs, deterministic=True)
```

**è¿è¡Œè¯„ä¼°**:
```bash
python project/scripts/evaluate_score_based.py \
  --model project/logs/ppo_score_based_v1_20260224_113524/final_model \
  --games 100 \
  --output project/experiments/evaluation/ppo_eval_new
```

### D. å‚è€ƒæ–‡çŒ®

1. Schulman et al. (2017). "Proximal Policy Optimization Algorithms." arXiv:1707.06347
2. Bravi et al. (2019). "Rinascimento: Reward Shaping for Board Games." (Splendor baseline)
3. Stable-Baselines3 Documentation. https://stable-baselines3.readthedocs.io/
4. Splendor Game Rules. Asmodee, 2014.

---

**æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ**: 2026-02-25  
**ç‰ˆæœ¬**: 1.0  
**è”ç³»**: Yehao Yan (IFT6759 Course Project)
